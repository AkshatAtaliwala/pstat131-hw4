---
title: "PSTAT 131 - Homework Assignment 4"
author: "Akshat Ataliwala (7924145)"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: pdf_document
---


```{r setup, echo=FALSE}
library(knitr)
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5)
options(digits = 4)


## indents are for indenting r code as formatted text
## They may need to be adjusted depending on your OS
# if your output looks odd, increase or decrease indent
indent1 = '    '
indent2 = '        '
indent3 = '            '
```

# Resampling

For this assignment, we will continue working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`data/titanic_codebook.txt`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

Make sure you load the `tidyverse` and `tidymodels`!

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

Create a recipe for this dataset **identical** to the recipe you used in Homework 3.

```{r message = FALSE}
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(corrr)
library(klaR)
library(MASS)
library(discrim)
library(poissonreg)
tidymodels_prefer()
data <- read_csv("data/titanic.csv")
data %>% head(5)
```

```{r}
data$survived <- as.factor(data$survived) 
data$survived <- relevel(data$survived, "Yes")
data$pclass <- as.factor(data$pclass)
data %>% head(5)
```

### 1. Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 

```{r}
set.seed(3478)

data_split <- initial_split(data, 
                            prop = 0.8,
                            strata = survived)

train <- training(data_split)
test <- testing(data_split)
```

```{r}
dim(data) 
0.8 * nrow(data)
dim(train)
dim(test)
```

### 2. Fold the training data. Use k-fold cross-validation, with k=10.
```{r}
folds <- vfold_cv(data = train, 
                  v = 10, 
                  strata = survived)
folds
```


### 3. In your own words, explain what we are doing in Question 2. What is k-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we did use the entire training set, what resampling method would that be?

K-fold cross validation is a technique used to avoid over fitting, and is used during the model training process. After we split our total data into training and testing, we want to evaluate the performance of our trained model, but showing it the test set would essentially be cheating the model. Instead, we further subdivide the training set into k folds, where each fold is essentially a training set and a validation set that is a subset of the training set. Each fold is different but from the same training set, and therefore we can use the validation set in each fold like a test set, and evaluate different hyperparameters and tune our model before we show the model the final test set. This will improve generalize-ability because we get to evaluate our model before it has seen the test set, because if we tune the model based on testing performance we can over fit to the test set. If we use the entire training set to resample, we would be bootstrapping.

### 4. Set up workflows for 3 models:

A logistic regression with the glm engine;
A linear discriminant analysis with the MASS engine;
A quadratic discriminant analysis with the MASS engine.
How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you’ll fit to each fold.

```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, 
                         data = train) %>% 
  step_impute_linear(age, impute_with = imp_vars(all_predictors())) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms = ~ sex_male:fare) %>% 
  step_interact(terms = ~ age:fare) 

```


```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wflow <- workflow() %>%
  add_recipe(titanic_recipe) %>%
  add_model(log_reg)
```


```{r}
lda_mod <- discrim_linear() %>%
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wflow <- workflow() %>%
  add_model(lda_mod) %>%
  add_recipe(titanic_recipe)
```


```{r}
qda_mod <- discrim_quad() %>%
  set_mode("classification") %>%
  set_engine("MASS")

qda_wflow <- workflow() %>%
  add_model(qda_mod) %>%
  add_recipe(titanic_recipe)
```

In total, we are fitting 30 models, 10 (1 sub-model / fold) for each of the 3 different models (logistic regression, lda, qda).

### 5. Fit each of the models created in Question 4 to the folded data.

*IMPORTANT: Some models may take a while to run – anywhere from 3 to 10 minutes. You should NOT re-run these models each time you knit. Instead, run them once, using an R script, and store your results; look into the use of loading and saving. You should still include the code to run them when you knit, but set eval = FALSE in the code chunks.*

```{r, message = FALSE, warning = FALSE}
degree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)
degree_grid

tune_res_log_reg <- tune_grid(
  object = log_wflow, 
  resamples = folds, 
  grid = degree_grid)

tune_res_lda <- tune_grid(
  object = lda_wflow, 
  resamples = folds, 
  grid = degree_grid)

tune_res_qda <- tune_grid(
  object = qda_wflow, 
  resamples = folds, 
  grid = degree_grid)
```

### 6. Use collect_metrics() to print the mean and standard errors of the performance metric accuracy across all folds for each of the four models. Decide which of the 3 fitted models has performed the best. Explain why. (Note: You should consider both the mean accuracy and its standard error.)

```{r}
# Logistics Regression Metrics
collect_metrics(tune_res_log_reg)
```

```{r}
# Linear Discriminant Analysis Metrics
collect_metrics(tune_res_lda)
```

```{r}
# Quadratic Discriminant Analysis Metrics
collect_metrics(tune_res_qda)
```

Looking at the models' performance after k-fold cross validation, its clear the qda is worse than lda and logistic regression across the board. The difference between logistic regression and lda is more subtle, as logisitic regression has a higher accuracy by 0.0042. However, the lda model has a lower standard error by 0.0029 and a higher roc_auc score by 0.006, so I will be choosing the lda model since it might perform slightly better with new data given the smaller error and larger ROC.

### 7. Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
lda_fit <- fit(lda_wflow, train)
```


### 8. Finally, with your fitted model, use predict(), bind_cols(), and accuracy() to assess your model’s performance on the testing data! Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.
```{r}
lda_predict <- predict(lda_fit, new_data = test)
predictions <- bind_cols(test %>% select(survived), lda_predict)
predictions
```

```{r}
lda_acc <- augment(lda_fit, new_data = test) %>%
  accuracy(truth = survived, estimate = .pred_class)
lda_acc
```
The lda model actually performed slightly better on the test set (0.7989) than it did on average with k-fold CV (0.7937). This is pretty good and means our model is generalizable in some sense, but it could probably be even better if we tuned various hyperparameters when cross validating and selecting a model because the accuracies are so similar (essentially meaning that the current cross validation didn't improve accuracy that much).
